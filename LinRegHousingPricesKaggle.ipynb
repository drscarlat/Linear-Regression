{
  "cells": [
    {
      "metadata": {
        "_uuid": "09f84a4f1f13aca8d9f52122f70a8523dc14e985"
      },
      "cell_type": "markdown",
      "source": "**HOUSE PRICES - Advanced Linear Regression**\n\n* In this notebook I've tried to keep ALL the imputing, transformations, scaling, etc - to be done via PIPELINE (using classes).\n* This allowed me to experiment quickly with different options, parameters, etc. \n* Found for example that manually eliminating features, using the feature importance tool with a class that drops columns - is better than PCA.\n* I cannot overestimate the importance of the Learning / Validation curves - it is the ONLY way to identify UNDER or OVERfitting.\n* With some tweaking you can achieve 12.24 on Kaggle with this notebook\n* But hey, play with it and let me know your thoughts and ideas on this notebook.\n______________________________________________________________________________________________________________________________________________________________________________\n* Many thanks to:\n* Serigne https://www.kaggle.com/serigne/stacked-regressions-top-4-on-leaderboard\n* MassQuantity https://www.kaggle.com/massquantity/all-you-need-is-pca-lb-0-11421-top-4\n* Their notebooks helped me a LOT ! Hopefully this notebook will help others as well."
    },
    {
      "metadata": {
        "_uuid": "1c474f7bdbbbeda031ed6c7fd85955a2e7be14ed"
      },
      "cell_type": "markdown",
      "source": "**Load modules**\n\n"
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from operator import itemgetter    \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nget_ipython().magic(u'matplotlib inline')\nplt.style.use('ggplot')\n\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.preprocessing import RobustScaler, StandardScaler, LabelEncoder, MinMaxScaler, OneHotEncoder, LabelBinarizer\nfrom sklearn.metrics import mean_squared_error, accuracy_score\nfrom sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion\nfrom scipy.stats import skew\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.base import clone, BaseEstimator, TransformerMixin\nfrom sklearn.cross_validation import KFold, cross_val_score\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, RandomizedSearchCV, KFold, cross_val_predict, StratifiedKFold, train_test_split, learning_curve, ShuffleSplit\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, SGDRegressor, BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, BaggingRegressor\nfrom sklearn.svm import SVR, LinearSVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nprint(\"Modules imported \\n\")\n\nprint(\"Files in current directory:\")\nimport os\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"../input\"]).decode(\"utf8\")) #check the files available in the directory\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "959933fad2d107b341de582b4157a46e3bf6e37d"
      },
      "cell_type": "markdown",
      "source": "**Load raw data**"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Load raw data\ntrain = pd.read_csv('../input/train.csv') \ntest = pd.read_csv('../input/test.csv') \n\n# Locally \n#train = pd.read_csv('/Users/Alex/Desktop/HousingLinReg/train.csv') \n#test = pd.read_csv('/Users/Alex/Desktop/HousingLinReg/test.csv') \nprint(\"train \", train.shape)\nprint(\"test \", test.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "b78b4c0cb0271b7ccd88ed8081dcefa1408cd3af"
      },
      "cell_type": "markdown",
      "source": "**Visualization**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "77923a8104064844bc237179f7e283cc32eb272c",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Histograms\ntrain.hist(bins=50, figsize=(20,15))\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "44e356f4851234f6dec490115af084bb3ff7cc3e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Check contents of ONE column\nprint(train[\"MSSubClass\"].value_counts())\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "04ce79c5e2c8418ed72cfa2b9847ba018b7a8a36",
        "scrolled": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Deleting outliers\n#plt.figure(figsize=(12,6))\n#plt.scatter(x=train.GrLivArea, y=train.SalePrice)\n#plt.xlabel(\"GrLivArea\", fontsize=13)\n#plt.ylabel(\"SalePrice\", fontsize=13)\n#plt.ylim(0,800000)\n\n#train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#plt.figure(figsize=(12,6))\n#plt.scatter(x=train.GrLivArea, y=train.SalePrice)\n#plt.xlabel(\"GrLivArea\", fontsize=13)\n#plt.ylabel(\"SalePrice\", fontsize=13)\n#plt.ylim(0,800000)\n\n#print(\"train without outliers \", train.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "225a78c609959204ae0cf8f7e3e2f9f3df57795e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Pearson Correlation Coefficient\ncorr_matrix = train.corr()\ncorr_matrix[\"SalePrice\"].sort_values(ascending=False)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "fabbb2e149eced67706e8f349ff8e6a957d1e15b",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Scatter_matrix\nattributes = [\"OverallQual\", \"YrSold\", \"YearBuilt\",\"SalePrice\"]\nscatter_matrix(train[attributes], figsize=(12, 8))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "be1208977c4b27feaf814c111855934462de9b6d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Zoom in on one plot\ntrain.plot(kind=\"scatter\", x=\"OverallQual\", y=\"SalePrice\",alpha=0.2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "06326ba7500e7059a007738e84aacddca6f15832"
      },
      "cell_type": "markdown",
      "source": "**Check for missing data**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b4ab861aacc87c3fd810ada7960a5cbf76d3412a",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Some transformation should be done on train + test\n# If not - there's a difference between the columns in train and test after get_dummies as there are different options in train vs test\n# Scaler - is important to be done separately as not to influence the mean and std of train with those of test, this leads to snooping on the test and overfitting\n\ntrainWprice = pd.DataFrame(train)\ntrainNoPrice = trainWprice.drop(\"SalePrice\", axis=1)\n\nfull=pd.concat([trainNoPrice,test], ignore_index=True)\nfull.drop(['Id'],axis=1, inplace=True)\n\nprint(\"train \", train.shape)\nprint(\"test \", test.shape)\nprint(\"full without Id and no SalePrice \", full.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a1d858c2a1d473c8346ad7cfe872e9c1462f916",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# ### Missing Data\nColsMissingValues = full.isnull().sum()\nprint(\"There are \", len(ColsMissingValues[ColsMissingValues>0]), \" features with missing values\")\n#print(\"_\"*80)\nall_data_na = (full.isnull().sum() / len(full)) * 100\nall_data_na = all_data_na.sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nprint(missing_data.head(len(ColsMissingValues[ColsMissingValues>0])))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "561f2c6c77537f2dff51d89c9713a144a7be4605"
      },
      "cell_type": "markdown",
      "source": "**Classes to be used by the pipeline**  with feature engineering: Imputing missing data, Adding / Removing features, Skew, PCA, Scaling"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d2a4a266452da4a73337eaafa733063ccc8099cd",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "class feat_eng(BaseEstimator, TransformerMixin):\n    def __init__(self, fill_missvals = True):\n        self.fill_missvals = fill_missvals\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.fill_missvals:\n            X[\"PoolQC\"] = X[\"PoolQC\"].fillna(\"None\")\n            X[\"MiscFeature\"] = X[\"MiscFeature\"].fillna(\"None\")\n            X[\"Alley\"] = X[\"Alley\"].fillna(\"None\")\n            X[\"Fence\"] = X[\"Fence\"].fillna(\"None\")\n            X[\"FireplaceQu\"] = X[\"FireplaceQu\"].fillna(\"None\")\n            X[\"LotFrontage\"] = X.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n            X['MSZoning'] = X['MSZoning'].fillna(X['MSZoning'].mode()[0])            \n            X[\"Functional\"] = X[\"Functional\"].fillna(\"Typ\")\n            X['Electrical'] = X['Electrical'].fillna(X['Electrical'].mode()[0])\n            X['KitchenQual'] = X['KitchenQual'].fillna(X['KitchenQual'].mode()[0])\n            X['Exterior1st'] = X['Exterior1st'].fillna(X['Exterior1st'].mode()[0])\n            X['Exterior2nd'] = X['Exterior2nd'].fillna(X['Exterior2nd'].mode()[0])\n            X['SaleType'] = X['SaleType'].fillna(X['SaleType'].mode()[0])\n            X['MSSubClass'] = X['MSSubClass'].fillna(\"None\")\n\n            for col in ('GarageType', 'GarageFinish', 'GarageQual', \n                        'GarageCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', \n                        'BsmtFinType1', 'BsmtFinType2','MasVnrType'):\n                X[col] = X[col].fillna('None')\n                \n            for col in ('GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'):\n                X[col] = X[col].fillna(0)\n                \n            for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n                X[col] = X[col].fillna(0)\n                \n            X['MSSubClass'] = X['MSSubClass'].apply(str) \n            X['OverallCond'] = X['OverallCond'].astype(str)\n            X['YrSold'] = X['YrSold'].astype(str)\n            X['MoSold'] = X['MoSold'].astype(str)\n            \n            X = X.drop(['Utilities'], axis=1)\n            \n        return X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "35bf3b17531e314d0cca9038fae0e016e412cb29"
      },
      "cell_type": "code",
      "source": "class add_feature(BaseEstimator, TransformerMixin):\n    def __init__(self,additional=1):\n        self.additional = additional\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.additional==1:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            #X[\"Age\"] = X[\"YrSold\"] - X[\"YearBuilt\"]\n        else:\n            X[\"TotalHouse\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"]   \n            X[\"TotalArea\"] = X[\"TotalBsmtSF\"] + X[\"1stFlrSF\"] + X[\"2ndFlrSF\"] + X[\"GarageArea\"]\n            #X[\"Age\"] = X[\"YrSold\"] - X[\"YearBuilt\"]\n\n        return X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "93454579a4cf4942fef1a4dc7d4dc0d3948b106d"
      },
      "cell_type": "code",
      "source": "class skew_dummies(BaseEstimator, TransformerMixin):\n    def __init__(self,skew=0.75):\n        self.skew = skew\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        X_numeric=X.select_dtypes(exclude=[\"object\"])\n        skewness = X_numeric.apply(lambda x: skew(x))\n        skewness_features = skewness[abs(skewness) >= self.skew].index\n        X[skewness_features] = np.log1p(X[skewness_features])\n        X = pd.get_dummies(X)\n        \n        return X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "9c2e68bb0eb5c54b3e465c87ec60763764afb025"
      },
      "cell_type": "code",
      "source": "class labelenc(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):      \n        lab = LabelEncoder()\n        \n        cols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\n        for c in cols:\n            X[c] = lab.fit_transform(X[c])\n        \n        return X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "2b412a97c948ca39ee8915e14550fdb5a5219e53"
      },
      "cell_type": "code",
      "source": "class grid():\n    def __init__(self,model):\n        self.model = model\n    \n    def grid_get(self,X,y,param_grid):\n        grid_search = GridSearchCV(self.model,param_grid,cv=5, scoring=\"neg_mean_squared_error\")\n        grid_search.fit(X,y)\n        print(grid_search.best_params_, np.sqrt(-grid_search.best_score_))\n        grid_search.cv_results_['mean_test_score'] = np.sqrt(-grid_search.cv_results_['mean_test_score'])\n        print(pd.DataFrame(grid_search.cv_results_)[['params','mean_test_score','std_test_score']])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "8236e769dcbb8c04e4379469f5924aa4ca20800d"
      },
      "cell_type": "code",
      "source": "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Error\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = 1-np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = 1-np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "d961e563763663394ac1bbffcbe8372862c76edf"
      },
      "cell_type": "code",
      "source": "class drop_cols(BaseEstimator, TransformerMixin):\n    def __init__(self, remove_cols = True):\n        self.remove_cols = remove_cols\n        pass\n    \n    def fit(self,X,y=None):\n        return self\n    \n    def transform(self,X):\n        if self.remove_cols:\n            del X['PoolQC']\n            #del X['YrSold']\n            #del X['YearBuilt']\n            del X['BsmtFinType1']\n            del X['LowQualFinSF']\n            del X['MoSold']\n            del X['Electrical']\n            del X['BldgType']\n            #del X['HouseStyle']\n            #del X['BsmtUnfSF']\n            #del X['BsmtFinType2']\n            #del X['BsmtFullBath']\n            #del X['BsmtExposure']\n            #del X['BsmtQual']\n            #del X['BsmtCond']\n            del X['SaleType']\n            del X['BsmtFinSF2']\n            #del X['HeatingQC']\n            #del X['Heating']\n            del X['Exterior2nd']\n            #del X['Foundation']\n            del X['ExterCond']\n            #del X['BedroomAbvGr']\n            del X['2ndFlrSF']\n            #del X['KitchenAbvGr']\n            #del X['RoofMatl']\n            del X['3SsnPorch']\n            del X['Exterior1st']\n            del X['MasVnrType']\n            #del X['ExterQual']\n            #del X['PavedDrive']\n            #del X['FireplaceQu']\n            #del X['Functional']\n            #del X['GarageType']\n            del X['GarageFinish']\n            #del X['MSSubClass']\n            del X['Alley']\n            #del X['LotShape']\n            del X['PoolArea']\n            #del X['LandContour']\n            #del X['Condition1']         \n            #del X['SaleCondition']\n            #del X['Condition2']\n            del X['RoofStyle']\n            del X['MiscFeature']\n            del X['Fence']\n            #del X['GarageCond']\n            #del X['GarageQual']\n            #del X['MSZoning']\n            #del X['Neighborhood']\n            del X['BsmtHalfBath']\n            del X['Street']\n            #del X['KitchenQual']\n            del X['LotConfig']\n            #del X['OverallQual']\n            #del X['FullBath']\n            #del X['TotRmsAbvGrd']\n            #del X['GarageArea']\n            #del X['LandSlope']\n            del X['TotalBsmtSF']\n            del X['GarageYrBlt']\n            #del X['LotFrontage']\n          \n        return X",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "2979e98bc697a9291aff1c384d13f000ffe80f66"
      },
      "cell_type": "code",
      "source": "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "c763f6c20c5d59cf5ec43a9b465a4d25168d5311"
      },
      "cell_type": "markdown",
      "source": "**PIPELINE**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "da992acaaffe615b79ec9641aa8467c8fdb3a08d",
        "scrolled": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Prepare the data, pipeline for models evaluation\n\n# PIPELINE\npipe = Pipeline([\n    ('feat_eng', feat_eng()),\n    ('add_feature', add_feature(additional=2)),\n    ('lab_enc', labelenc()), \n    ('drop_cols', drop_cols()), \n    ('skew_dummies', skew_dummies(skew=1)), \n    \n    ])\n###                   ** skew_dummies is taking care of the labelencoder AND onehotencoder with get_dummies() !  **\n\ntrain = pd.read_csv('../input/train.csv') \ntest = pd.read_csv('../input/test.csv') \n\ntrainWprice = pd.DataFrame(train)\n#trainWprice = trainWprice.drop(trainWprice[(trainWprice['GrLivArea']>4000) & (trainWprice['SalePrice']<300000)].index)\ntrainNoPrice = trainWprice.drop(\"SalePrice\", axis=1)\n\nfull=pd.concat([trainNoPrice,test], ignore_index=True)\nfull.drop(['Id'],axis=1, inplace=True)\nprint(\"full without Id and no price \", full.shape)\n\nFullDataPipe = pipe.fit_transform(full)\nprint(\"FullDataPipe \", FullDataPipe.shape)\n\nn_train=train.shape[0]\ntrainFinal = pd.DataFrame(FullDataPipe[:n_train])\ntestFinal = pd.DataFrame(FullDataPipe[n_train:])\ny= train.SalePrice\nyFinal = np.log(train.SalePrice)\n\n# Scaler should be run separately on train and test to prevent overfitting\nscaler = RobustScaler()\ntrainFinal = scaler.fit_transform(trainFinal)\ntestFinal = scaler.fit_transform(testFinal)\n\n# PCA should be run separately on train and test \n#pca = PCA(n_components = 0.999) \n# Check the number of feats that will keep 99.9% variance. Note the number may be different for train vs test\n#pca = PCA(n_components = 200) \n#trainFinal = pca.fit_transform(trainFinal)\n#testFinal = pca.fit_transform(testFinal)\n\nprint(\"trainFinal\", trainFinal.shape)\nprint(\"testFinal\", testFinal.shape)\nprint(\"yFinal\", yFinal.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": false,
        "_uuid": "c11437724870b3856e6a2ba28317b09ef00fa721",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# FEATURE IMPORTANCE - Needs its own SEPARATE pipeline without Scaler or PCA in order to see the features' NAMES and not their numbers\n# Useful even AFTER PCA - check the relevance of features for prediction\n\ntrainFinalFI = pd.DataFrame(trainFinal)\nyFinalFI = yFinal\n\nlasso=Lasso(alpha=0.001, copy_X=True, fit_intercept=True, max_iter=1000,\n   normalize=False, positive=False, precompute=False, random_state=None,\n   selection='cyclic', tol=0.0001, warm_start=False)\nlasso.fit(trainFinalFI,yFinalFI)\n\nFI_lasso = pd.DataFrame({\"Feature Importance\":lasso.coef_}, index=trainFinalFI.columns)\n\n# Focus on those with 0 importance\n#print(FI_lasso.sort_values(\"Feature Importance\",ascending=False).to_string())\n#print(\"_\"*80)\nFI_lasso[FI_lasso[\"Feature Importance\"]!=0].sort_values(\"Feature Importance\").plot(kind=\"barh\",figsize=(15,25))\nplt.xticks(rotation=90)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "6302bbb7dfc291ac46d27f74777ef1c3e3aa14d0",
        "scrolled": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# CHECK if any Missing Data\ntrainFinal = pd.DataFrame(trainFinal)\n\nColsMissingValues = trainFinal.isnull().sum()\nprint(\"There are \", len(ColsMissingValues[ColsMissingValues>0]), \" features with missing values\")\n#print(\"_\"*80)\nall_data_na = (trainFinal.isnull().sum() / len(trainFinal)) * 100\nall_data_na = all_data_na.sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nprint(missing_data.head(len(ColsMissingValues[ColsMissingValues>0])))\nprint(\"_\"*80)\nprint(\"trainFinal \", trainFinal.shape)\nprint(\"yFinal \", yFinal.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8be0776ac73f18c58ff9535c6b0292ffd3f489fd"
      },
      "cell_type": "markdown",
      "source": "**Cross Validation**"
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "8c29db6fefc62bdc135a6405b4ccc7491dd185ce"
      },
      "cell_type": "code",
      "source": "# define cross validation \ndef rmse_cv(model,X,y):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=5))\n    return rmse",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "0bd63e7604e1a83a4f1765b9c17cfe36883649e8"
      },
      "cell_type": "markdown",
      "source": "**MODELS**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "43ec1898e87cb77b9cdd665712f1c0c139c87a96",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Lin reg ALL 14 models HYPERPARAMS NOT optimized\n#models = [LinearRegression(),Ridge(),Lasso(),RandomForestRegressor(),GradientBoostingRegressor(),SVR(),LinearSVR(),\n#          ElasticNet(),SGDRegressor(),BayesianRidge(),KernelRidge(),ExtraTreesRegressor(),XGBRegressor(),lgb.LGBMRegressor()]\n#names = [\"LR\", \"Ridge\", \"Lasso\", \"RF\", \"GBR\", \"SVR\", \"LinSVR\", \"Ela\",\"SGD\",\"Bay\",\"Ker\",\"Extra\",\"Xgb\", \"LightGBM\"]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4f1499cb79042d4e39bbdf64d793dce39cc0d038",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Some initial models with Hyper params optimized\nmodels = [\n    Lasso(alpha =0.0005, random_state=1),\n    ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3),\n    KernelRidge(alpha=2.2, coef0=2.5, degree=3, gamma=None, kernel='polynomial',kernel_params=None),\n    SVR(C=2, cache_size=200, coef0=0.1, degree=3, epsilon=0.005, gamma=0.005,\n  kernel='rbf', max_iter=-1, shrinking=True, tol=0.01, verbose=False),\n    GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,\n             learning_rate=0.05, loss='huber', max_depth=5,\n             max_features='sqrt', max_leaf_nodes=None,\n             min_impurity_decrease=0.0, min_impurity_split=None,\n             min_samples_leaf=10, min_samples_split=10,\n             min_weight_fraction_leaf=0.0, n_estimators=2000,\n             n_iter_no_change=None, presort='auto', random_state=None,\n             subsample=1.0, tol=0.0001, validation_fraction=0.1, verbose=0,\n             warm_start=False),\n    #XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n    #                         learning_rate=0.05, max_depth=3, \n    #                         min_child_weight=1.7817, n_estimators=2200,\n    #                         reg_alpha=0.4640, reg_lambda=0.8571,\n    #                         subsample=0.5213, silent=1,\n    #                         random_state =7, nthread = -1),\n    #lgb.LGBMRegressor(objective='regression',num_leaves=5,\n    #                          learning_rate=0.05, n_estimators=720,\n    #                          max_bin = 55, bagging_fraction = 0.8,\n    #                          bagging_freq = 5, feature_fraction = 0.2319,\n    #                          feature_fraction_seed=9, bagging_seed=9,\n    #                          min_data_in_leaf =6, min_sum_hessian_in_leaf = 11),\n    \n         ]\n\nnames = [\"LASSO\", \"ELA\",\"KER\", \"SVR \", \"GBR\" ]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "edc981a5a8094f696373000c110031696df59ee9",
        "scrolled": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Run the models and compare\nModScores = {}\n\nfor name, model in zip(names, models):\n    score = rmse_cv(model, trainFinal, yFinal)\n    ModScores[name] = score.mean()\n    print(\"{}: {:.6f}\".format(name,score.mean()))\n\nprint(\"trainFinal \", trainFinal.shape)\nprint(\"_\"*80)\nfor key, value in sorted(ModScores.items(), key = itemgetter(1), reverse = False):\n    print(key, value)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ba46186cec4bd1803c6f80e04a484d385c2f23b3"
      },
      "cell_type": "markdown",
      "source": "**Hyper params optimization of the  models with Grid Search**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4c8ce51bb13556f3f2b675221ad53e18311895ab",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# SEARCH GRID FOR HYPERPARAMS OPTIMIZATION\n\n# SVR\nparam_grid = [{'kernel': [\"rbf\"], 'degree': [3]},\n              {'gamma': [0.005],'coef0': [0.1,0.05],'tol': [0.01],\n              'C': [2.5,2.2],'epsilon': [0.005],},]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a8eccce153df36565feeb502d8fa33e07ff6e2c7",
        "scrolled": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Grid Search Optimization for models\nmodel4cv = SVR()\n\ngrid_search = GridSearchCV(model4cv, param_grid)\ngrid_search.fit(trainFinal, yFinal)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)\nprint(\"_\"*80)\nprint(grid(grid_search.best_estimator_).grid_get(trainFinal, yFinal,{}))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "707893b0b9a9781699b152422c6a53d9a19e8262",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Optimized hyper params for models\nlasso = Lasso(alpha =0.0005, random_state=1)\n\nker = KernelRidge(alpha=2.2, coef0=2.5, degree=3, gamma=None, kernel='polynomial',\n      kernel_params=None)\n\nsvr = SVR(C=2, cache_size=200, coef0=0.1, degree=3, epsilon=0.005, gamma=0.005,\n  kernel='rbf', max_iter=-1, shrinking=True, tol=0.01, verbose=False)\n\ngbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nxgb = XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nlgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nela = ElasticNet(alpha=0.01, copy_X=True, fit_intercept=True, l1_ratio=0.05,\n      max_iter=5000, normalize=False, positive=False, precompute=False,\n      random_state=None, selection='cyclic', tol=0.0001, warm_start=False)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "69f7f25c23fff6c06ff108e9503489cd59132f4e"
      },
      "cell_type": "markdown",
      "source": "**Final model fit, evaluation & prediction**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5c33a778bae50b085acdd59f317f09bcd6625b34",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Final model fit, evaluation & prediction\n\nmodel = SVR(C=2, cache_size=200, coef0=0.1, degree=3, epsilon=0.005, gamma=0.005,\n  kernel='rbf', max_iter=-1, shrinking=True, tol=0.01, verbose=False)\n#model = AveragingModels(models = (svr, ker))\n#model = AveragingModels(models = (ela, svr, ker, gbr, lasso, xgb, lgb))\n\nmodel.fit(trainFinal, yFinal)\nscore = rmse_cv(model, trainFinal, yFinal)\nprint(\" model score: {:.5f} ({:.4f})\\n\".format(score.mean(), score.std()))\n\npred = np.exp(model.predict(testFinal))\npred = np.around(pred, decimals=4, out=None)\nprint(pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "961bd57cd150916cd5c998498fef315290d7150d"
      },
      "cell_type": "markdown",
      "source": "**Learning / Validation Curves - Jtrain vs Jcv to help identify under or overfitting**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "4a9ab63f2e50797ae65b02475894f1731ffb557b",
        "scrolled": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# LEARNING CURVE\nX, y = trainFinal, yFinal\nestimator = model\n\ntitle = \"Learning Curves (SVR)\"\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nplot_learning_curve(estimator, title, X, y, ylim=(0.01, 0.17), cv=cv, n_jobs=4)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "80a44fc6ae4864d9eb84fca9cc30f5ac16208054"
      },
      "cell_type": "markdown",
      "source": "**SUBMISSION**"
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "a3052d404bec3a8c3cf86fb89518f871e179807e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# SUBMISSION\nresult=pd.DataFrame({'Id':test.Id, 'SalePrice':pred})\nresult.to_csv(\"submission.csv\",index=False)\nprint(result)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}